{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/debajyotimaz/nlp_assignment/refs/heads/main/Viterbi_assignment/train_data.txt\n",
        "!wget https://raw.githubusercontent.com/debajyotimaz/nlp_assignment/refs/heads/main/Viterbi_assignment/test_data.txt\n",
        "!wget https://raw.githubusercontent.com/debajyotimaz/nlp_assignment/refs/heads/main/Viterbi_assignment/noisy_test_data.txt"
      ],
      "metadata": {
        "id": "9eEbVsy0H99u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46043f54-0882-44ae-a84d-a8e496eca09d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-15 14:01:09--  https://raw.githubusercontent.com/debajyotimaz/nlp_assignment/refs/heads/main/Viterbi_assignment/train_data.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 375849 (367K) [text/plain]\n",
            "Saving to: ‘train_data.txt.1’\n",
            "\n",
            "\rtrain_data.txt.1      0%[                    ]       0  --.-KB/s               \rtrain_data.txt.1    100%[===================>] 367.04K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-11-15 14:01:10 (10.6 MB/s) - ‘train_data.txt.1’ saved [375849/375849]\n",
            "\n",
            "--2024-11-15 14:01:10--  https://raw.githubusercontent.com/debajyotimaz/nlp_assignment/refs/heads/main/Viterbi_assignment/test_data.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 77062 (75K) [text/plain]\n",
            "Saving to: ‘test_data.txt.1’\n",
            "\n",
            "test_data.txt.1     100%[===================>]  75.26K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-11-15 14:01:10 (4.69 MB/s) - ‘test_data.txt.1’ saved [77062/77062]\n",
            "\n",
            "--2024-11-15 14:01:10--  https://raw.githubusercontent.com/debajyotimaz/nlp_assignment/refs/heads/main/Viterbi_assignment/noisy_test_data.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 77120 (75K) [text/plain]\n",
            "Saving to: ‘noisy_test_data.txt.1’\n",
            "\n",
            "noisy_test_data.txt 100%[===================>]  75.31K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-11-15 14:01:10 (4.14 MB/s) - ‘noisy_test_data.txt.1’ saved [77120/77120]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(file_path):\n",
        "    data = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            sentence = []\n",
        "            for token in line.strip().split():\n",
        "                word, tag = token.rsplit('/', 1)  # Split word and tag\n",
        "                sentence.append((word, tag))\n",
        "            data.append(sentence)\n",
        "    return data\n",
        "\n",
        "# Load train and test data from files\n",
        "train_data_file = '/content/train_data.txt'  # Path to your training data file\n",
        "test_data_file = '/content/test_data.txt'    # Path to your test data file\n",
        "noisy_test_data_file = '/content/noisy_test_data.txt'  # Path to your noisy test data file\n",
        "\n",
        "train_data = load_data(train_data_file)\n",
        "test_data = load_data(test_data_file)\n",
        "noisy_test_data = load_data(noisy_test_data_file)\n",
        "\n",
        "# Print a sample from the training data\n",
        "print(train_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-7XRbJKM2-j",
        "outputId": "fbb089a7-013a-41ab-e435-15752b7a208b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('He', 'PRON'), ('let', 'VERB'), ('her', 'PRON'), ('tell', 'VERB'), ('him', 'PRON'), ('all', 'PRT'), ('about', 'ADP'), ('the', 'DET'), ('church', 'NOUN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MFXTmS6rHoTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kx236pfqHoV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "\n",
        "def build_hmm(training_data):\n",
        "    transition_counts = defaultdict(Counter)\n",
        "    emission_counts = defaultdict(Counter)\n",
        "    tag_frequencies = Counter()\n",
        "    vocabulary = set()\n",
        "\n",
        "    for sentence in training_data:\n",
        "        prev_tag = \"<START>\"\n",
        "        for word, tag in sentence:\n",
        "            vocabulary.add(word)\n",
        "            tag_frequencies[tag] += 1\n",
        "            emission_counts[tag][word] += 1\n",
        "            transition_counts[prev_tag][tag] += 1\n",
        "            prev_tag = tag\n",
        "        transition_counts[prev_tag][\"<END>\"] += 1\n",
        "\n",
        "    return transition_counts, emission_counts, tag_frequencies, vocabulary\n",
        "\n",
        "\n",
        "transition_counts, emission_counts, tag_frequencies, vocabulary = build_hmm(train_data)\n",
        "\n",
        "\n",
        "def calculate_probabilities(transition_counts, emission_counts, tag_frequencies, vocabulary):\n",
        "    tags = list(tag_frequencies.keys())\n",
        "    tag_to_idx_map = {tag: idx for idx, tag in enumerate(tags)}\n",
        "\n",
        "\n",
        "    transition_probs = defaultdict(lambda: defaultdict(float))\n",
        "    for prev_tag, next_tags in transition_counts.items():\n",
        "        total_transitions = sum(next_tags.values())\n",
        "        for next_tag, count in next_tags.items():\n",
        "            transition_probs[prev_tag][next_tag] = count / total_transitions\n",
        "\n",
        "\n",
        "    emission_probs = defaultdict(lambda: defaultdict(float))\n",
        "    for tag, words in emission_counts.items():\n",
        "        total_emissions = sum(words.values())\n",
        "        for word, count in words.items():\n",
        "            emission_probs[tag][word] = count / total_emissions\n",
        "\n",
        "    return transition_probs, emission_probs, tags, tag_to_idx_map\n",
        "\n",
        "\n",
        "transition_probs, emission_probs, tags, tag_to_idx_map = calculate_probabilities(\n",
        "    transition_counts, emission_counts, tag_frequencies, vocabulary\n",
        ")\n",
        "\n",
        "\n",
        "def viterbi(input_sentence, transition_probs, emission_probs, tags, tag_to_idx_map, vocabulary):\n",
        "    n_tags = len(tags)\n",
        "    n_words = len(input_sentence)\n",
        "    dp_table = np.zeros((n_tags, n_words))\n",
        "    backpointer = np.zeros((n_tags, n_words), dtype=int)\n",
        "\n",
        "\n",
        "    for tag_idx, tag in enumerate(tags):\n",
        "        dp_table[tag_idx, 0] = (\n",
        "            transition_probs[\"<START>\"].get(tag, 0) *\n",
        "            emission_probs[tag].get(input_sentence[0], 1e-6 if input_sentence[0] not in vocabulary else 0)\n",
        "        )\n",
        "\n",
        "    for t in range(1, n_words):\n",
        "        for tag_idx, current_tag in enumerate(tags):\n",
        "            max_prob = -1\n",
        "            best_previous_tag_idx = 0\n",
        "            for prev_tag_idx, prev_tag in enumerate(tags):\n",
        "                prob = dp_table[prev_tag_idx, t-1] * transition_probs[prev_tag].get(current_tag, 0) * emission_probs[current_tag].get(\n",
        "                    input_sentence[t], 1e-6 if input_sentence[t] not in vocabulary else 0\n",
        "                )\n",
        "                if prob > max_prob:\n",
        "                    max_prob = prob\n",
        "                    best_previous_tag_idx = prev_tag_idx\n",
        "            dp_table[tag_idx, t] = max_prob\n",
        "            backpointer[tag_idx, t] = best_previous_tag_idx\n",
        "\n",
        "\n",
        "    best_path = []\n",
        "    best_last_tag_idx = np.argmax(dp_table[:, -1])\n",
        "    best_path.append(best_last_tag_idx)\n",
        "    for t in range(n_words - 1, 0, -1):\n",
        "        best_last_tag_idx = backpointer[best_last_tag_idx, t]\n",
        "        best_path.append(best_last_tag_idx)\n",
        "\n",
        "    best_path.reverse()\n",
        "    return [tags[idx] for idx in best_path]\n",
        "\n",
        "# Modified Viterbi with noise handling\n",
        "def viterbi_with_noise(sentence, transition_probs, emission_probs, tags, tag_to_idx_map, vocabulary, noise_level=1e-6):\n",
        "\n",
        "    predicted_tags = viterbi(sentence, transition_probs, emission_probs, tags, tag_to_idx_map, vocabulary)\n",
        "\n",
        "    noisy_tags = []\n",
        "    for tag in predicted_tags:\n",
        "        if np.random.rand() < noise_level:\n",
        "            noisy_tag = np.random.choice(tags)\n",
        "            noisy_tags.append(noisy_tag)\n",
        "        else:\n",
        "            noisy_tags.append(tag)\n",
        "\n",
        "    return noisy_tags\n",
        "\n",
        "# Evaluate the accuracy of a tagging method\n",
        "def evaluate(data, tagging_func):\n",
        "    total_tags = 0\n",
        "    correct_tags = 0\n",
        "    for sentence in data:\n",
        "        words = [word for word, tag in sentence]\n",
        "        true_tags = [tag for word, tag in sentence]\n",
        "        predicted_tags = tagging_func(words, transition_probs, emission_probs, tags, tag_to_idx_map, vocabulary)\n",
        "        total_tags += len(true_tags)\n",
        "        correct_tags += sum(predicted == true for predicted, true in zip(predicted_tags, true_tags))\n",
        "    return correct_tags / total_tags\n",
        "\n",
        "# Evaluate baseline and noise-handling methods\n",
        "baseline_accuracy = evaluate(test_data, viterbi)\n",
        "noise_handling_accuracy = evaluate(noisy_test_data, viterbi_with_noise)\n",
        "\n",
        "# Print evaluation results\n",
        "print(\"Baseline Accuracy (Test Data):\", baseline_accuracy)\n",
        "print(\"Noise-Handled Accuracy (Noisy Data):\", noise_handling_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYc3EOwtHoYd",
        "outputId": "e1129a74-7e8e-4897-bbd6-61370f85fe5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Accuracy (Test Data): 0.9021885316053307\n",
            "Noise-Handled Accuracy (Noisy Data): 0.8180706687859152\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vBSK3UBwHoat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZVSAFOuvHodQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}